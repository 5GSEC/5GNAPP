import os
import operator
from typing import TypedDict, Annotated, List, Literal
from langchain_core.messages import BaseMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import tool, AgentExecutor
from langchain.memory import ConversationBufferWindowMemory
from pydantic import BaseModel, Field
from IPython.display import display, Image

from langgraph.prebuilt import create_react_agent
from langgraph_supervisor import create_supervisor
from langgraph.graph import StateGraph, START, MessagesState, END
from langgraph.types import Command

from sdl_apis import *
from mitre_apis import *
from control_apis import *
from prompts import *
from utils import *

from langchain_core.messages import convert_to_messages

# Init LLM settings
if not os.getenv("GOOGLE_API_KEY"):
    print("Warning: GOOGLE_API_KEY not found in environment variables.")
    print("Please set it for the LangChain Gemini LLM to work.")

gemini_llm_model = "gemini-2.5-flash-preview-04-17"
llm = ChatGoogleGenerativeAI(model=gemini_llm_model, temperature=0.3)


# Define the Langgraph state
class MobiLLMState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        alerts: Raw security alerts and event data from network components.
        network_data: Additional contextual network data for analysis.
        threat_summary: A concise summary of the detected security threat, generated by an LLM.
        threat_category: The classified category of the threat, based on MITRE ATT&CK.
        mitre_technique: The specific MITRE FiGHT / ATT&CK technique ID related to the threat.
        countermeasures: Suggested countermeasures for the identified threat.
    """
    query: str
    event: str
    network_data: str
    threat_summary: str #Annotated[str, operator.setitem]
    threat_category: str #Annotated[str, operator.setitem]
    mitre_technique: str #Annotated[str, operator.setitem]
    countermeasures: str #Annotated[str, operator.setitem]
    chat_response: str
    task: str

def supervisor(state: MobiLLMState) -> MobiLLMState:
    query = state["query"]
    if "[chat]" in query:
        state["task"] = "chat"
    elif "[security analysis]" in query:
        state["task"] = "security_analysis"
    else:
        raise ValueError("Router received empty input.")
    return state

###################### MobiLLM Chat Agent ######################

# tools that can be used by the chat agent
mobillm_chat_tools = [
    get_ue_mobiflow_data_all_tool,
    get_ue_mobiflow_data_by_index_tool,
    get_ue_mobiflow_description_tool,
    get_bs_mobiflow_data_all_tool,
    get_bs_mobiflow_data_by_index_tool,
    get_bs_mobiflow_description_tool,
    fetch_sdl_event_data_all_tool,
    fetch_sdl_event_data_by_ue_id_tool,
    fetch_sdl_event_data_by_cell_id_tool,
    get_event_description_tool,
    fetch_service_status_tool,
    build_xapp_tool,
    deploy_xapp_tool,
    unDeploy_xapp_tool,
]

# --- MobiLLM Chat Agent Setup ---
chat_prompt_str = BASE_REACT_PROMPT_TEMPLATE_STR.format(TASK_BACKGROUND=DEFAULT_CHAT_TASK_BACKGROUND)
mobillm_chat_agent = create_react_agent(model=llm, tools=mobillm_chat_tools, prompt=DEFAULT_CHAT_TASK_BACKGROUND, name="mobillm_chat_agent")

def mobillm_chat_agent_node(state: MobiLLMState) -> MobiLLMState:
    query = state["query"]
    response = mobillm_chat_agent.invoke({"messages": [("user", query)]})["messages"][-1].content
    state["chat_response"] = response
    return state

###################### MobiLLM Security analysis agent ######################

mobillm_security_analysis_tools = [
    get_ue_mobiflow_data_all_tool,
    get_ue_mobiflow_data_by_index_tool,
    get_ue_mobiflow_description_tool,
    get_bs_mobiflow_data_all_tool,
    get_bs_mobiflow_data_by_index_tool,
    get_bs_mobiflow_description_tool,
    fetch_sdl_event_data_all_tool,
    fetch_sdl_event_data_by_ue_id_tool,
    fetch_sdl_event_data_by_cell_id_tool,
    get_event_description_tool,
]

mobillm_security_analysis_agent = create_react_agent(model=llm, tools=mobillm_security_analysis_tools, prompt=DEFAULT_SECURITY_ANLYSIS_TASK_BACKGROUND, name="mobillm_security_analysis_agent")

def mobillm_security_analysis_agent_node(state: MobiLLMState) -> MobiLLMState:
    query = state["query"]
    response = mobillm_security_analysis_agent.invoke({"messages": [("user", query)]})["messages"][-1].content
    state["threat_summary"] = response
    return state

###################### MobiLLM Security Classification agent ######################

mobillm_security_analysis_tools = [
    get_all_mitre_fight_techniques,
    get_mitre_fight_technique_by_id,
]

###################### Building Graph ######################

builder = StateGraph(MobiLLMState)
builder.add_node("supervisor", supervisor)
builder.add_node("mobillm_chat_agent", mobillm_chat_agent_node)
builder.add_node("mobillm_security_analysis_agent", mobillm_security_analysis_agent_node)

builder.add_edge(START, "supervisor")
builder.add_conditional_edges(
    "supervisor",
    lambda state: state["task"],
    {
        "chat": "mobillm_chat_agent",
        "security_analysis": "mobillm_security_analysis_agent"
    }
)

graph = builder.compile()

# input_state = {"query": "[chat] How many services are currently in Running state and how long they have been running?"}
# input_state = {"query": "[chat] How many cells are currently deployed in the network?"}
input_state = {"query": "[security analysis] Generate a detailed threat analysis report for event ID 1"}
result = graph.invoke(input_state)
print(result)  # Should return chatbot response

