import os
import operator
import re
import json
from typing import TypedDict, Annotated, List, Literal
from langchain_core.messages import BaseMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import tool, AgentExecutor
from langchain.memory import ConversationBufferWindowMemory
from pydantic import BaseModel, Field
from IPython.display import display, Image

from langgraph.prebuilt import create_react_agent
from langgraph_supervisor import create_supervisor
from langgraph.graph import StateGraph, START, MessagesState, END
from langgraph.types import Command, interrupt
from langgraph.checkpoint.memory import InMemorySaver

from sdl_apis import *
from mitre_apis import *
from control_apis import *
from prompts import *
from utils import *

from langchain_core.messages import convert_to_messages

# Init LLM settings
if not os.getenv("GOOGLE_API_KEY"):
    print("Warning: GOOGLE_API_KEY not found in environment variables.")
    print("Please set it for the LangChain Gemini LLM to work.")

gemini_llm_model = "gemini-2.5-flash" # "gemini-2.5-flash-preview-04-17"
llm = ChatGoogleGenerativeAI(model=gemini_llm_model, temperature=0.3)


# Define the Langgraph state
class MobiLLMState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        alerts: Raw security alerts and event data from network components.
        network_data: Additional contextual network data for analysis.
        threat_summary: A concise summary of the detected security threat, generated by an LLM.
        mitre_technique: The specific MITRE FiGHT / ATT&CK technique ID related to the threat.
        countermeasures: Suggested countermeasures for the identified threat.
    """
    query: str
    event: str
    network_data: str
    threat_summary: str #Annotated[str, operator.setitem]
    mitre_technique: str #Annotated[str, operator.setitem]
    countermeasures: str #Annotated[str, operator.setitem]
    actionable: Literal["yes", "no"]
    action_strategy: Literal["config tuning", "reboot", "none"]
    action_plan: str
    chat_response: str
    task: str
    updated_config: str
    outcome: str
    tools_called: List[str]

def supervisor(state: MobiLLMState) -> MobiLLMState:
    query = state["query"]
    if "[chat]" in query:
        state["task"] = "chat"
    elif "[security analysis]" in query:
        state["task"] = "security_analysis"
    else:
        raise ValueError("Router received empty input.")
    return state

###################### MobiLLM Chat Agent ######################

# tools that can be used by the chat agent
mobillm_chat_tools = [
    get_ue_mobiflow_data_all_tool,
    get_ue_mobiflow_data_by_index_tool,
    get_ue_mobiflow_description_tool,
    get_bs_mobiflow_data_all_tool,
    get_bs_mobiflow_data_by_index_tool,
    get_bs_mobiflow_description_tool,
    fetch_sdl_event_data_all_tool,
    fetch_sdl_event_data_by_ue_id_tool,
    fetch_sdl_event_data_by_cell_id_tool,
    get_event_description_tool,
    fetch_service_status_tool,
    build_xapp_tool,
    deploy_xapp_tool,
    unDeploy_xapp_tool,
]

# --- MobiLLM Chat Agent Setup ---
chat_prompt_str = BASE_REACT_PROMPT_TEMPLATE_STR.format(TASK_BACKGROUND=DEFAULT_CHAT_TASK_BACKGROUND)
mobillm_chat_agent = create_react_agent(model=llm, tools=mobillm_chat_tools, prompt=DEFAULT_CHAT_TASK_BACKGROUND, name="mobillm_chat_agent")

def mobillm_chat_agent_node(state: MobiLLMState) -> MobiLLMState:
    query = state["query"]
    if query is None or query.strip() == "":
        return state
    call_result = mobillm_chat_agent.invoke({"messages": [("user", query)]})
    response = call_result["messages"][-1].content
    state["chat_response"] = response
    state["tools_called"] = state["tools_called"] + call_result["messages"][1].tool_calls
    return state

###################### MobiLLM Security analysis agent ######################

mobillm_security_analysis_tools = [
    get_ue_mobiflow_data_all_tool,
    get_ue_mobiflow_data_by_index_tool,
    get_ue_mobiflow_description_tool,
    get_bs_mobiflow_data_all_tool,
    get_bs_mobiflow_data_by_index_tool,
    get_bs_mobiflow_description_tool,
    fetch_sdl_event_data_all_tool,
    fetch_sdl_event_data_by_ue_id_tool,
    fetch_sdl_event_data_by_cell_id_tool,
    get_event_description_tool,
]

mobillm_security_analysis_agent = create_react_agent(model=llm, tools=mobillm_security_analysis_tools, prompt=DEFAULT_SECURITY_ANLYSIS_TASK_BACKGROUND, name="mobillm_security_analysis_agent")

def mobillm_security_analysis_agent_node(state: MobiLLMState) -> MobiLLMState:
    query = state["query"]
    if query is None or query.strip() == "":
        return state
    call_result = mobillm_security_analysis_agent.invoke({"messages": [("user", query)]})
    response = call_result["messages"][-1].content
    state["threat_summary"] = response
    state["tools_called"] = state["tools_called"] + call_result["messages"][1].tool_calls
    return state

###################### MobiLLM Security Classification agent ######################

mobillm_security_classification_tools = [
    get_all_mitre_fight_techniques,
    get_mitre_fight_technique_by_id,
    search_mitre_fight_techniques,
]

mobillm_security_classification_agent = create_react_agent(model=llm, tools=mobillm_security_classification_tools, prompt=DEFAULT_SECURITY_CLASSIFICATION_TASK_BACKGROUND, name="mobillm_security_classification_agent")

def mobillm_security_classification_agent_node(state: MobiLLMState) -> MobiLLMState:
    threat_summary = state["threat_summary"]
    if threat_summary is None or threat_summary.strip() == "":
        return state
    call_result = mobillm_security_classification_agent.invoke({"messages": [("user", threat_summary)]})
    response = call_result["messages"][-1].content
    state["mitre_technique"] = response
    state["tools_called"] = state["tools_called"] + call_result["messages"][1].tool_calls
    return state

###################### MobiLLM Security Response agent ######################

mobillm_security_response_tools = [
    get_all_mitre_fight_techniques,
    get_mitre_fight_technique_by_id,
    get_ran_cu_config_tool,
    update_ran_cu_config_tool,
    reboot_ran_cu_tool,
]

mobillm_security_response_agent = create_react_agent(model=llm, tools=mobillm_security_response_tools, prompt=DEFAULT_SECURITY_RESPONSE_TASK_BACKGROUND, name="mobillm_security_response_agent")

def mobillm_security_response_agent_node(state: MobiLLMState) -> MobiLLMState:
    threat_summary = state["threat_summary"]
    mitre_technique = state["mitre_technique"]
    if threat_summary is None or threat_summary.strip() == "":
        return state
    if mitre_technique is None or mitre_technique.strip() == "":
        return state

    call_result = mobillm_security_response_agent.invoke({"messages": [("user", f"Threat summary:\n{threat_summary}\nRelevant MiTRE FiGHT Techniques:\n{mitre_technique}")]})
    
    print(call_result)
    
    raw_response = call_result["messages"][-1].content

    # ensure the response is json formatted string
    try:
        response = raw_response.strip().replace("\n", "")
        response = json.loads(response)  # Try direct parse first
    except json.JSONDecodeError:
        # Fallback: Extract with regex and try to parse
        json_match = re.search(r'{[\s\S]*}', response)
        if json_match:
            try:
                response = json.loads(json_match.group())
            except json.JSONDecodeError:
                print("Error: Unable to parse the response as JSON.")
                response = ""
    
    if response != "":
        state["actionable"] = response["actionable"] # should be "yes" or "no"
        state["action_plan"] = response["action_plan"]
        state["action_strategy"] = response["action_strategy"]
    else:
        state["actionable"] = "no"
        state["action_plan"] = ""
        state["action_strategy"] = "none"

    state["countermeasures"] = response
    state["tools_called"] = state["tools_called"] + call_result["messages"][1].tool_calls
    return state

###################### MobiLLM Config tuning agent ######################

mobillm_config_tuning_tools = [
    get_all_mitre_fight_techniques,
    get_mitre_fight_technique_by_id,
    get_ran_cu_config_tool,
    update_ran_cu_config_tool,
    reboot_ran_cu_tool,
]

mobillm_config_tuning_agent = create_react_agent(model=llm, tools=mobillm_config_tuning_tools, prompt=DEFAULT_CONFIG_TUNING_TASK_BACKGROUND, name="mobillm_config_tuning_agent")

def mobillm_config_tuning_agent_node(state: MobiLLMState) -> MobiLLMState:
    actionable = state["actionable"] # should be "yes" or "no"
    action_plan = state["action_plan"]
    action_strategy = state["action_strategy"]

    if actionable.lower() != "yes" or action_strategy != "config tuning" or action_plan == "": 
        # ensure the action plan and action strategy are matched
        print("No actionable plan provided.")
        return state
    
    call_result = mobillm_config_tuning_agent.invoke({"messages": [("user", f"Action plan:\n{action_plan}")]})
    raw_response = call_result["messages"][-1].content

    print("\nconfig tuning raw_response", raw_response)

    # ensure the response is json formatted string
    try:
        response = raw_response.strip().replace("\n", "")
        response = json.loads(response)  # Try direct parse first
    except json.JSONDecodeError:
        # Fallback: Extract with regex and try to parse
        json_match = re.search(r'{[\s\S]*}', response)
        if json_match:
            try:
                response = json.loads(json_match.group())
            except json.JSONDecodeError:
                print("Error: Unable to parse the response as JSON.")
                response = ""
    
    if response != "":
        state["actionable"] = response["actionable"] # should be "yes" or "no"
        state["outcome"] = response["outcome"]
        state["updated_config"] = response["updated_config"]

    state["tools_called"] = state["tools_called"] + call_result["messages"][1].tool_calls

    return state

###################### Building Graph ######################

builder = StateGraph(MobiLLMState)
builder.add_node("supervisor", supervisor)
builder.add_node("mobillm_chat_agent", mobillm_chat_agent_node)
builder.add_node("mobillm_security_analysis_agent", mobillm_security_analysis_agent_node)
builder.add_node("mobillm_security_classification_agent", mobillm_security_classification_agent_node)
builder.add_node("mobillm_security_response_agent", mobillm_security_response_agent_node)
builder.add_node("mobillm_config_tuning_agent", mobillm_config_tuning_agent_node)

builder.add_edge(START, "supervisor")
builder.add_edge("mobillm_security_analysis_agent", "mobillm_security_classification_agent")
builder.add_edge("mobillm_security_classification_agent", "mobillm_security_response_agent")

builder.add_conditional_edges(
    "supervisor",
    lambda state: state["task"],
    {
        "chat": "mobillm_chat_agent",
        "security_analysis": "mobillm_security_analysis_agent"
    }
)

def route_after_response_agent(state):
    if state["actionable"].strip() == "yes" and state["action_strategy"].strip() == "config tuning":
        return "config_tuning"
    else:
        return "end"

builder.add_conditional_edges(
    "mobillm_security_response_agent",
    route_after_response_agent,
    {
        "config_tuning": "mobillm_config_tuning_agent",
        "end": END
    }
)

checkpointer = InMemorySaver()

config = {"configurable": {"thread_id": "1"}}
graph = builder.compile(checkpointer=checkpointer)

image_data = graph.get_graph().draw_mermaid_png()

# Write to a local file
with open("mobillm_langgraph.png", "wb") as f:
    f.write(image_data)

# input_state = {"query": "[chat] How many services are currently in Running state and how long they have been running?", "tools_called": []}
# input_state = {"query": "[chat] How many cells are currently deployed in the network?", "tools_called": []}
input_state = {"query": "[security analysis] Conduct a thorough security analysis for event ID 1", "tools_called": []}
result = graph.invoke(input_state, config=config)
# print(result)

# resume after interruption for update_ran_cu_config_tool
user_input = input(f'Approve the tool call?\n{result["__interrupt__"][0].value}\nYour option (yes/edit/no):')
resume_command = {"type": "accept"}
if user_input == "yes":
    resume_command = {"type": "accept"}
elif user_input == "no":
    resume_command = {"type": "deny"}
result = graph.invoke(Command(resume=resume_command), config=config)

# resume after interruption for reboot CU
user_input = input(f'Approve the tool call?\n{result["__interrupt__"][0].value}\nYour option (yes/edit/no):')
resume_command = {"type": "accept"}
if user_input == "yes":
    resume_command = {"type": "accept"}
elif user_input == "no":
    resume_command = {"type": "deny"}
result = graph.invoke(Command(resume=resume_command), config=config)


if "chat_response" in result:
    print("Chat Response:", result["chat_response"])
    print("\n\n")
if "threat_summary" in result:
    print("Threat Summary:", result["threat_summary"])
    print("\n\n")
if "mitre_technique" in result:
    print("MITRE Technique:", result["mitre_technique"])
    print("\n\n")
if "countermeasures" in result:
    print("Countermeasures:", result["countermeasures"])
    print("\n\n")
if "outcome" in result:
    print("Outcome:", result["outcome"])
    print("\n\n")
if "tools_called" in result:
    print("Tools Called:")
    for tool in result["tools_called"]:
        print(tool)
    print("\n\n")
